# -*- coding: utf-8 -*-
"""ML_FINAL_PROJ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQqORICqoFLxz9g4VOZCrI_PjMK4_Hiw
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset recruitment_data.csv. This dataset shall be utilised to assess the accuracy of our Cloud Based ML Candidate Hiring Prediction Model.
file_path = 'recruitment_data.csv'
data = pd.read_csv(file_path)

# Remove rows with missing values. To remove bias, skewness that could affect the overall profile of a candidate.
data = data.dropna()

# Remove outliers. Inconsistent or misleading data could lead to incorrect interpretations of a candidate being hired, which can affect the overall evaluation.
def remove_outliers(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df[col] = df[col].clip(lower_bound, upper_bound)
    return df

data = remove_outliers(data, ['Age', 'ExperienceYears', 'PreviousCompanies', 'DistanceFromCompany', 'InterviewScore', 'SkillScore', 'PersonalityScore'])

# Standardize the data
scaler = StandardScaler()
data[['Age', 'ExperienceYears', 'PreviousCompanies', 'DistanceFromCompany', 'InterviewScore', 'SkillScore', 'PersonalityScore']] = scaler.fit_transform(data[['Age', 'ExperienceYears', 'PreviousCompanies', 'DistanceFromCompany', 'InterviewScore', 'SkillScore', 'PersonalityScore']])

# Plot histograms for each feature against HiringDecision. Highly valuable to compare the classes of prediction leaning towards hired / non hired candidates.
features = ['Age', 'ExperienceYears', 'PreviousCompanies', 'DistanceFromCompany', 'InterviewScore', 'SkillScore', 'PersonalityScore']
for feature in features:
    plt.figure()
    data[data['HiringDecision'] == 0][feature].hist(alpha=0.5, color='blue', bins=20, label='Not Hired')
    data[data['HiringDecision'] == 1][feature].hist(alpha=0.5, color='green', bins=20, label='Hired')
    plt.legend(loc='upper right')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.title(f'Histogram of {feature} by Hiring Decision')
    plt.show()

# Handle class imbalance using SMOTE. Highly beneficial towards exhibiting the accuracy of the ensemble model in the real world towards
# hiring / non hiring candidates. The criteria to handle these imbalances include Accuracy, Precision, Recall and F1 Score.
X = data.drop('HiringDecision', axis=1)
y = data['HiringDecision']
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split the data. To assess scalability, quality and overfitting to determine the Cloud Based ML Candidate Hiring Prediction Model's response to real world data.
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Feature selection with SelectKBest
# SelectKBest is used to select the top 10 features based on their statistical significance specific to hiring a candidate
select_k_best = SelectKBest(f_classif, k=10)

# Pipelines for each classifier to ensure a clean workflow with reproducibility. The models are RandomForestClassifier, SupportVectorClassifier,
# LogisticRegression and KNeighborsClassifier.
pipeline_rfc = Pipeline([
    ('scaler', scaler),
    ('feature_selection', select_k_best),
    ('classifier', RandomForestClassifier(random_state=42))
])

pipeline_svc = Pipeline([
    ('scaler', scaler),
    ('feature_selection', select_k_best),
    ('classifier', SVC(probability=True, random_state=42))
])

pipeline_lr = Pipeline([
    ('scaler', scaler),
    ('feature_selection', select_k_best),
    ('classifier', LogisticRegression(random_state=42))
])

pipeline_knn = Pipeline([
    ('scaler', scaler),
    ('feature_selection', select_k_best),
    ('classifier', KNeighborsClassifier())
])

# List of models to evaluate for the basis of ensemble modelling to assess and enhance the accuracy.
models = [
    ('Random Forest', pipeline_rfc),
    ('Support Vector Classifier', pipeline_svc),
    ('Logistic Regression', pipeline_lr),
    ('K-Nearest Neighbors', pipeline_knn)
]

# Function to evaluate and plot results for our Cloud Based ML Hiring Prediction Model. An emphasis is laid on Model Training & Prediction to compute the
# desired Performance Metrics, perform Comprehensive Manual Error Analysis and utilise the ROC curve for sensitising data.
def evaluate_classification_model(model, X_train, X_test, y_train, y_test, model_name):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    error_rate = 1 - accuracy

    print(f"{model_name} - Accuracy: {accuracy}")
    print(f"{model_name} - Precision: {precision}")
    print(f"{model_name} - Recall: {recall}")
    print(f"{model_name} - F1 Score: {f1}")
    print(f"{model_name} - Error Rate: {error_rate}")

    if y_prob is not None:
        mse = mean_squared_error(y_test, y_prob)
        r2 = r2_score(y_test, y_prob)
        print(f"{model_name} - Mean Squared Error: {mse}")
        print(f"{model_name} - RÂ² Score: {r2}")

        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        plt.figure()
        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'{model_name} - Receiver Operating Characteristic')
        plt.legend(loc="lower right")
        plt.show()

    # Confusion Matrix to eliminate out unsuitable versus suitable candidates.
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.title(f'{model_name} - Confusion Matrix')
    plt.show()

# Evaluate each of our classification models above, where we determine insights to assess the technical quality respective to the metrics being utilized.
for name, model in models:
    print(f"\n{name}:")
    evaluate_classification_model(model, X_train, X_test, y_train, y_test, name)

# Feature importance for Random Forest. This is particularly important for gauging the behavior, business insights and revenue in the real world.
rfc = pipeline_rfc.named_steps['classifier']
if hasattr(rfc, 'feature_importances_'):
    importances = rfc.feature_importances_
    indices = np.argsort(importances)[::-1]
    features = X.columns[indices]

    plt.figure()
    plt.title("Feature Importances - Random Forest")
    plt.bar(range(X.shape[1]), importances[indices], align="center")
    plt.xticks(range(X.shape[1]), features, rotation=90)
    plt.xlim([-1, X.shape[1]])
    plt.show()